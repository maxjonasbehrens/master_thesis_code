{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jcCwLDmWkrHi"
      },
      "source": [
        "# Script to PreProcess Data and Train CNNs in Google Colab\n",
        "\n",
        "## PreProcess Images and feed into CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KnRwnXgRkXTH",
        "outputId": "6a9fb642-3377-4025-f832-7f7899df82c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Lfk-ya7Ux0k",
        "outputId": "d83ae34d-216f-4412-ce26-868bb53743db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Install package to handle .tiff data\n",
        "!pip install pyrsgis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5AhCi0KFdG",
        "colab_type": "code",
        "outputId": "61751331-2e7c-4c13-8dfe-fac7f8f5d449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Clone my git repo for preprocessing the images\n",
        "!git clone https://github.com/maxjonasbehrens/master_thesis_code\n",
        "#%cd /content/master_thesis_code\n",
        "#!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbcpkAIYKSkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Python script for preprocessing\n",
        "%load /content/master_thesis_code/NeuralNetTrain/imgProcessing/sat_images.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CwZd9v4Ukxt6",
        "outputId": "7c24ccac-9e9f-4079-e548-629068f33932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# Load necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import pyrsgis\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import scipy.io\n",
        "import sklearn.model_selection\n",
        "import imageio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoaE3NmFOOAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import own module to preprocess images\n",
        "import imp\n",
        "sat_images = imp.new_module('sat_images')\n",
        "exec(open(\"/content/master_thesis_code/NeuralNetTrain/imgProcessing/sat_images.py\").read(), sat_images.__dict__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4zPKrdH71D",
        "colab_type": "text"
      },
      "source": [
        "# Create input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1NURzseNL_yH",
        "outputId": "5c992505-52b5-4a14-8e45-8f6c1ab82548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Load Data Frame of NUTS regions and corresponding gdp values\n",
        "y_dat = pd.read_csv(\"/gdrive/My Drive/ThesisData/Data/enhanced_gdp_data.csv\")\n",
        "y_dat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ogm4ndTFkrib",
        "colab": {}
      },
      "source": [
        "# List of all files in directory of images\n",
        "mypath = \"/gdrive/My Drive/nuts_viirs/\"\n",
        "\n",
        "# PreProcess, split and save image data\n",
        "sat_images.create_save_data(mypath,y_dat,prediction='nuts_value',kind='normal',replace_nan='mean',resolution=1024,night=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5bvu7lRgpgS",
        "colab_type": "text"
      },
      "source": [
        "## Fit Linear Model for Benchmarking Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3xDa2R4gohV",
        "colab_type": "code",
        "outputId": "beb4dbb2-c711-4b4d-fd46-2cc3ff069bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load libraries\n",
        "from os import listdir\n",
        "from os.path import isfile, join, getsize\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Define image paths\n",
        "train_path = \"/gdrive/My Drive/ThesisData/training/viirs_night/\"\n",
        "val_path = \"/gdrive/My Drive/ThesisData/validation/viirs_night/\"\n",
        "test_path = \"/gdrive/My Drive/ThesisData/test/viirs_night/\"\n",
        "\n",
        "# Get images\n",
        "train_files = [f for f in listdir(train_path) if isfile(join(train_path, f))]\n",
        "val_files = [f for f in listdir(val_path) if isfile(join(val_path, f))]\n",
        "test_files = [f for f in listdir(test_path) if isfile(join(test_path, f))]\n",
        "\n",
        "# Get GDP data\n",
        "y_dat = pd.read_csv(\"/gdrive/My Drive/ThesisData/Data/enhanced_gdp_data.csv\")\n",
        "#test_f = val_files + test_files\n",
        "test_f = test_files\n",
        "\n",
        "# Empty lists\n",
        "X_train = []\n",
        "X_test = []\n",
        "y_train_diff = []\n",
        "y_test_diff = []\n",
        "y_train_abs = []\n",
        "y_test_abs = []\n",
        "\n",
        "# Iterate through all training files and get information\n",
        "for f in train_files:\n",
        "  region = f.rsplit('_')[0]\n",
        "  year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
        "  mean = float(f.rsplit('_')[1])\n",
        "  \n",
        "  # Append average light and GDP data to list\n",
        "  X_train.append(mean)\n",
        "  y_train_diff.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_diff'].values[0])\n",
        "  y_train_abs.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_value'].values[0])\n",
        "\n",
        "# Iterate through all training files and get information\n",
        "for f in test_f:\n",
        "  region = f.rsplit('_')[0]\n",
        "  year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
        "  mean = float(f.rsplit('_')[1])\n",
        "\n",
        "  # Append average light and GDP data to list\n",
        "  X_test.append(mean)\n",
        "  y_test_diff.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_diff'].values[0])\n",
        "  y_test_abs.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_value'].values[0])\n",
        "\n",
        "# Convert to numpy array\n",
        "X_train = np.array([X_train])\n",
        "X_test = np.array([X_test])\n",
        "y_train = np.array([y_train])\n",
        "y_test = np.array([y_test])\n",
        "\n",
        "print('Train X: ',str(X_train[0][:3]))\n",
        "print('Test X: ',str(X_test[0][:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdCvDA_-5MBH",
        "colab_type": "code",
        "outputId": "68446514-9ecf-4416-ee49-006412b6d137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Swap axes to correct format\n",
        "X_train = np.swapaxes(X_train,0,-1)\n",
        "X_test = np.swapaxes(X_test,0,-1)\n",
        "y_train_abs = np.swapaxes(y_train_abs,0,-1)\n",
        "y_test_abs = np.swapaxes(y_test_abs,0,-1)\n",
        "y_train_diff = np.swapaxes(y_train_diff,0,-1)\n",
        "y_test_diff = np.swapaxes(y_test_diff,0,-1)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGPIPG1n2dC-",
        "colab_type": "code",
        "outputId": "342f8464-f8a9-4a1c-ee24-68c5ec517139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Perfom linear regression on relative GDPs\n",
        "reg = LinearRegression().fit(X_train, y_train_diff)\n",
        "r_square = reg.score(X_train, y_train_diff)\n",
        "print('R Square Difference: ',str(r_square))\n",
        "\n",
        "# Make predictions\n",
        "pred = reg.predict(X_test)\n",
        "\n",
        "# Compute prediction accuracy\n",
        "mse = np.mean((y_test_diff-pred)**2)\n",
        "mae = np.mean(np.abs(y_test_diff-pred))\n",
        "print('MSE Difference: ',str(mse))\n",
        "print('MAE Difference: ',str(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK7KIqWap5lg",
        "colab_type": "code",
        "outputId": "2b51d69f-eadd-4fed-db58-a35c7a1bac11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Perfom linear regression on relative GDPs\n",
        "reg = LinearRegression().fit(X_train, y_train_abs)\n",
        "r_square = reg.score(X_train, y_train_abs)\n",
        "print('R Square Absolute: ',str(r_square))\n",
        "\n",
        "# Make predictions\n",
        "pred = reg.predict(X_test)\n",
        "\n",
        "# Compute prediction accuracy\n",
        "mse = np.mean((y_test_abs-pred)**2)\n",
        "mae = np.mean(np.abs(y_test_abs-pred))\n",
        "print('MSE Absolute: ',str(mse))\n",
        "print('MAE Absolute: ',str(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c50RrTy6IHnt",
        "colab_type": "text"
      },
      "source": [
        "## Sample CNN Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y3Xdhdryg5R",
        "colab_type": "code",
        "outputId": "0c08bc4b-d3c4-4de3-c13d-f5f3fdb670fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load libraries\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import GaussianDropout\n",
        "from keras.layers import GaussianNoise\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from keras.optimizers import SGD, Adam, Nadam, Adamax, Adadelta, RMSprop\n",
        "from keras.applications import VGG19, ResNet50, ResNet50V2, InceptionResNetV2, MobileNetV2\n",
        "from keras.callbacks import EarlyStopping\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "from os import listdir\n",
        "import itertools\n",
        "from os.path import isfile, join, getsize\n",
        "#Image.MAX_IMAGE_PIXELS = 213437725"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig2E8Zgp4RnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_shape = (224,224,3)\n",
        "out_shape = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI39p13ywdCj",
        "colab_type": "code",
        "outputId": "b8f50daa-af90-4bd4-fd3b-cffbfa8b7208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# Very shallow CNN\n",
        "def xs_model(in_shape = (224,224,3), out_shape=1):\n",
        "    xs_model = Sequential()\n",
        "\n",
        "    xs_model.add(Conv2D(32, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
        "    xs_model.add(Flatten())\n",
        "    #xs_model.add(Dense(128, activation='relu'))\n",
        "    xs_model.add(Dense(units=out_shape,activation='linear'))\n",
        "    \n",
        "    return xs_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAOgPKmfZUfN",
        "colab_type": "code",
        "outputId": "681bfd55-534f-4f72-cc5e-6d419f0fa5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "# Very shallow CNN\n",
        "def s_model(in_shape = (224,224,3),out_shape=1):\n",
        "    s_model = Sequential()\n",
        "\n",
        "    s_model.add(Conv2D(32, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
        "    s_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    s_model.add(BatchNormalization())\n",
        "    s_model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "    s_model.add(Flatten())\n",
        "    s_model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
        "    s_model.add(Dropout(0.5))\n",
        "    s_model.add(Dense(units=out_shape,activation='linear'))\n",
        "    \n",
        "    return s_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L86HWD1sWhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Med Shallow Model\n",
        "def m_model(in_shape=(224,224,3),out_shape=1):\n",
        "    m_model = Sequential()\n",
        "\n",
        "    m_model.add(Conv2D(16, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
        "    m_model.add(Conv2D(16, 3, padding='same', activation='relu'))\n",
        "    m_model.add(BatchNormalization())\n",
        "    m_model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "    m_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    m_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    m_model.add(BatchNormalization())\n",
        "    m_model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "    m_model.add(Flatten())\n",
        "    m_model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
        "    m_model.add(Dropout(0.5))\n",
        "\n",
        "    m_model.add(Dense(256, activation='relu',activity_regularizer=l1(0.001)))\n",
        "    m_model.add(Dropout(0.5))\n",
        "    m_model.add(Dense(units=out_shape,activation='linear'))\n",
        "    \n",
        "    return m_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNEUN3yv3BDF",
        "colab_type": "code",
        "outputId": "030df3bd-310e-4026-ae1e-c09c81a13a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Transfer Learning Mobile Model\n",
        "def mobile_model(in_shape=(224,224,3),out_shape=1):\n",
        "    base_model = MobileNetV2(input_shape=in_shape,include_top=False)\n",
        "\n",
        "    x=base_model.output\n",
        "    x=Flatten()(x)\n",
        "    x=Dense(32,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "    x=Dropout(0.2)(x) #Dropout\n",
        "    x=Dense(32,activation='relu')(x) #dense layer 3\n",
        "    x=Dropout(0.2)(x) #Dropout\n",
        "    preds=Dense(units=out_shape, activation = 'linear')(x)\n",
        "    mobile_model=Model(inputs=base_model.input,outputs=preds)\n",
        "\n",
        "    for layer in mobile_model.layers[:-5]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    return mobile_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpKmZY683B5U",
        "colab_type": "code",
        "outputId": "23d7155a-5550-47ce-b952-1f8d3d52683d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Transfer Learning Inception Model\n",
        "def inception_model(in_shape=(224,224,3),out_shape=1):\n",
        "    base_model = InceptionResNetV2(input_shape=in_shape,include_top=False)\n",
        "\n",
        "    x=base_model.output\n",
        "    x=Flatten()(x)\n",
        "    x=Dense(32,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "    x=Dropout(0.2)(x) #Dropout\n",
        "    x=Dense(32,activation='relu')(x) #dense layer 3\n",
        "    x=Dropout(0.2)(x) #Dropout\n",
        "    preds=Dense(units=out_shape, activation = 'linear')(x)\n",
        "    inception_model=Model(inputs=base_model.input,outputs=preds)\n",
        "    \n",
        "    return inception_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQnkjk-Jje67",
        "colab_type": "text"
      },
      "source": [
        "# Training Pipeline for CNN Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6S35UIOJN9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot histogram of image size in different sets\n",
        "train_path = \"/gdrive/My Drive/ThesisData/training/viirs_night/\"\n",
        "val_path = \"/gdrive/My Drive/ThesisData/validation/viirs_night/\"\n",
        "test_path = \"/gdrive/My Drive/ThesisData/test/viirs_night/\"\n",
        "\n",
        "train_files = [getsize(join(train_path, f)) for f in listdir(train_path) if isfile(join(train_path, f))]\n",
        "val_files = [getsize(join(val_path, f)) for f in listdir(val_path) if isfile(join(val_path, f))]\n",
        "test_files = [getsize(join(test_path, f)) for f in listdir(test_path) if isfile(join(test_path, f))]\n",
        "\n",
        "pyplot.hist(train_files,bins=1000)\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.hist(val_files,bins=1000)\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.hist(test_files,bins=1000)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JzEKWvkUjTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create DataFrame to load the data\n",
        "def data_load(data_type, pred = 'nuts_value'):\n",
        "  \n",
        "  train_path = \"/gdrive/My Drive/ThesisData/training/\"+str(data_type)+\"/\"\n",
        "  val_path = \"/gdrive/My Drive/ThesisData/validation/\"+str(data_type)+\"/\"\n",
        "  test_path = \"/gdrive/My Drive/ThesisData/test/\"+str(data_type)+\"/\"\n",
        "\n",
        "  train_files = [f for f in listdir(train_path) if isfile(join(train_path, f)) and getsize(join(train_path, f)) > 100]\n",
        "  val_files = [f for f in listdir(val_path) if isfile(join(val_path, f)) and getsize(join(val_path, f)) > 100]\n",
        "  test_files = [f for f in listdir(test_path) if isfile(join(test_path, f)) and getsize(join(test_path, f)) > 100]\n",
        "\n",
        "  if pred == 'diff':\n",
        "    y_dat = pd.read_csv(\"/gdrive/My Drive/ThesisData/Data/gdp_time_series.csv\")\n",
        "  else:\n",
        "    y_dat = pd.read_csv(\"/gdrive/My Drive/ThesisData/Data/enhanced_gdp_data.csv\")\n",
        "\n",
        "  train_y = []\n",
        "  val_y = []\n",
        "  test_y = []\n",
        "\n",
        "\n",
        "  # Get the paths and GDP values for training data\n",
        "  for f in train_files:\n",
        "    if pred == 'diff':\n",
        "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
        "      region = f.rsplit('_')[0]\n",
        "      train_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
        "    else:\n",
        "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
        "      region = f.rsplit('_')[0]\n",
        "      train_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
        "\n",
        "  # Get the paths and GDP values for validation data\n",
        "  for f in val_files:\n",
        "    if pred == 'diff':\n",
        "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
        "      region = f.rsplit('_')[0]\n",
        "      val_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
        "    else:\n",
        "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
        "      region = f.rsplit('_')[0]\n",
        "      val_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
        "\n",
        "  # Get the paths and GDP values for test data\n",
        "  for f in test_files:\n",
        "    if pred == 'diff':\n",
        "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
        "      region = f.rsplit('_')[0]\n",
        "      test_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
        "    else:\n",
        "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
        "      region = f.rsplit('_')[0]\n",
        "      test_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
        "\n",
        "  # Scale the output GDP data\n",
        "  train_y = np.array(train_y)\n",
        "  train_y = train_y.reshape(len(train_y),1)\n",
        "  val_y = np.array(val_y)\n",
        "  val_y = val_y.reshape(len(val_y),1)\n",
        "  test_y = np.array(test_y)\n",
        "  test_y = test_y.reshape(len(test_y),1)\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(train_y)\n",
        "  train_y = scaler.transform(train_y)\n",
        "  val_y = scaler.transform(val_y)\n",
        "  test_y = scaler.transform(test_y)\n",
        "  \n",
        "  # Put info into DF so that Keras can read in the data automatically\n",
        "  train_df = pd.DataFrame({'y':train_y[:,0].tolist(),'file':train_files})\n",
        "  val_df = pd.DataFrame({'y':val_y[:,0].tolist(),'file':val_files})\n",
        "  test_df = pd.DataFrame({'y':test_y[:,0].tolist(),'file':test_files})\n",
        "\n",
        "  # Return DF per split and the scaler for later\n",
        "  return train_df, val_df, test_df, scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIsyGb6Qx2uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model\n",
        "def define_model(lr,in_shape, out_shape,optim,model):\n",
        "\t\n",
        "    # Define new model\n",
        "    mod = Sequential()\n",
        "    if model == 'xs_model':\n",
        "        mod = xs_model(in_shape=in_shape)\n",
        "    elif model == 's_model':\n",
        "        mod = s_model(in_shape=in_shape)\n",
        "    elif model == 'm_model':\n",
        "        mod = m_model(in_shape=in_shape)\n",
        "    elif model == 'inception_model':\n",
        "        mod = inception_model(in_shape=in_shape)\n",
        "    else:\n",
        "        mod = mobile_model(in_shape=in_shape)\n",
        "        \n",
        "    \n",
        "    # compile model with different optimising functions and learning rates\n",
        "    if optim == 'adam':\n",
        "        opt = Adam(lr=lr)\n",
        "    elif optim == 'sgd':\n",
        "        opt = SGD(lr=lr)\n",
        "    elif optim == 'rmsprob':\n",
        "        opt = RMSprop(lr=lr, rho=0.9)\n",
        "    else:\n",
        "        opt = Adamax(lr=lr)\n",
        "    \n",
        "    # Compile the model\n",
        "    mod.compile(optimizer=opt, loss='mean_squared_error', metrics=['mse','mae'])\n",
        " \n",
        "    return mod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg3xSQBLyujR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history,no_layers,in_shape,batch_size,epochs,mse,mae,transfer,lr,optim,comment,data_type,pred):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Model MSE')\n",
        "\tpyplot.plot(history.history['mean_squared_error'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_mean_squared_error'], color='orange', label='val')\n",
        "\tpyplot.xlabel(\"\")\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Model MAE')\n",
        "\tpyplot.plot(history.history['mean_absolute_error'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_mean_absolute_error'], color='orange', label='val')\n",
        "\t# save plot to file\n",
        "\tday = date.today()\n",
        "\tfilename = str(day)+\"_\"+str(in_shape[0])+\"_\"+str(in_shape[2])+\"_\"+str(epochs)+\"_\"+str(batch_size)\n",
        "\t#pyplot.savefig('/gdrive/My Drive/ThesisData/cnn_results/'+filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "  \n",
        "\t# write diagnostics to results file\n",
        "\tmyrow = ['\\n'+str(day),str(no_layers),str(batch_size),str(epochs),str(in_shape[0]),str(in_shape[2]),\n",
        "\t         str(history.history['mse'][-1]),str(history.history['mae'][-1]),\n",
        "\t\t\t\t\t     str(history.history['val_mse'][-1]),str(history.history['val_mae'][-1]),\n",
        "\t\t\t\t\t\t\t   str(mse),str(mae),str(transfer),str(lr),str(optim),str(comment),str(data_type),str(pred)]\n",
        "\t\n",
        "\t# Write to file\n",
        "\tmyrow = ','.join(myrow)\n",
        "\tfilepath = '/gdrive/My Drive/ThesisData/cnn_results/cnn_results.csv'\n",
        "\twith open(filepath,'a') as fd:\n",
        "\t\tfd.write(myrow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B8Hp4kcz3yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness(comment,transfer,optim,model,data_type,lr=0.0001,batch_size = 10,epochs = 50, target_size = 128, pred = 'nuts_value'):\n",
        "  \n",
        "  # load dataset\n",
        "  train_df, val_df, test_df = data_load(data_type=data_type, pred = pred)\n",
        "  \n",
        "  # create data generator\n",
        "  train_datagen = ImageDataGenerator()#rescale=(1.0/256))\n",
        "  val_datagen = ImageDataGenerator()#rescale=(1.0/256))\n",
        "  test_datagen = ImageDataGenerator()#rescale=(1.0/256))\n",
        "\n",
        "  # Define early stopping\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=30, restore_best_weights=True)\n",
        "\n",
        "  # File Directories\n",
        "  train_path = \"/gdrive/My Drive/ThesisData/training/\"+str(data_type)\n",
        "  val_path = \"/gdrive/My Drive/ThesisData/validation/\"+str(data_type)\n",
        "  test_path = \"/gdrive/My Drive/ThesisData/test/\"+str(data_type)\n",
        "\n",
        "  # Infer color mode and input shape\n",
        "  if 'day' in data_type or 'night' in data_type or data_type == 'subsample':\n",
        "    color_mode = 'rgb'\n",
        "    in_shape = (target_size,target_size,3)\n",
        "  else:\n",
        "    color_mode = 'rgba'\n",
        "    in_shape = (target_size,target_size,4)\n",
        "\n",
        "  print(in_shape)\n",
        "\n",
        "  # prepare iterators\n",
        "  train_it = train_datagen.flow_from_dataframe(train_df,\n",
        "                                               directory=train_path,\n",
        "                                               x_col = 'file',\n",
        "                                               y_col = 'y',\n",
        "                                               target_size = (target_size,target_size),\n",
        "                                               batch_size = batch_size,\n",
        "                                               class_mode = 'raw',\n",
        "                                               color_mode = color_mode)\n",
        "  \n",
        "  val_it = val_datagen.flow_from_dataframe(val_df, \n",
        "                                           directory=val_path,\n",
        "                                           x_col = 'file',\n",
        "                                           y_col = 'y',\n",
        "                                           target_size = (target_size,target_size),\n",
        "                                           batch_size = batch_size,\n",
        "                                           class_mode = 'raw',\n",
        "                                           color_mode = color_mode)\n",
        "  \n",
        "  test_it = test_datagen.flow_from_dataframe(test_df, \n",
        "                                             directory=test_path,\n",
        "                                             x_col = 'file',\n",
        "                                             y_col = 'y',\n",
        "                                             target_size = (target_size,target_size),\n",
        "                                             batch_size = batch_size,\n",
        "                                             class_mode = 'raw',\n",
        "                                             color_mode = color_mode)\n",
        "  \n",
        "  # define model\n",
        "  transfer = model\n",
        "  print('Model: '+model)\n",
        "  model = define_model(lr=lr,in_shape=in_shape,out_shape=1,optim=optim,model=model)\n",
        "  \n",
        "  # Get number of layers\n",
        "  no_layers = len(model.layers)\n",
        "  \n",
        "  # fit model\n",
        "  history = model.fit_generator(train_it, \n",
        "                                steps_per_epoch=len(train_it),\n",
        "                                validation_data=val_it, \n",
        "                                validation_steps=len(val_it),\n",
        "                                callbacks = [es], \n",
        "                                epochs=epochs, \n",
        "                                verbose=2)\n",
        "  \n",
        "  # evaluate model on unscaled GDP values\n",
        "  test_preds = model.predict_generator(test_it, steps=len(test_it), verbose=0)\n",
        "  # Inverse transformation to get unscaled predictions\n",
        "  unscaled_preds = scaler.inverse_transform(test_preds)\n",
        "  \n",
        "  # Get true GDP values\n",
        "  true_vals = test_it.labels\n",
        "  # Inverse transformation for true values\n",
        "  true_vals = scaler.inverse_transform(true_vals)\n",
        "  \n",
        "  # Compute unscaled MSE and MAE\n",
        "  mse = np.mean((true_vals-unscaled_preds)**2)\n",
        "  mae = np.mean(abs(true_vals-unscaled_preds))\n",
        "  print('> mse=%.3f, mae=%.3f' % (mse, mae))\n",
        "  \n",
        "  # learning curves\n",
        "  summarize_diagnostics(history=history,\n",
        "                        no_layers=no_layers,\n",
        "                        in_shape = in_shape,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=len(history.history['loss']),\n",
        "                        mse=mse,mae=mae,\n",
        "                        transfer=transfer,lr=lr,\n",
        "                        optim=optim,\n",
        "                        comment=comment,\n",
        "                        data_type=data_type,\n",
        "                        pred = pred)\n",
        "  \n",
        "  return model, test_it, scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT5HZlld6eLT",
        "colab_type": "code",
        "outputId": "ddda626c-1345-4862-be53-3444abf0ed17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Prepare iteration with all models, optimiser, and learning rates\n",
        "models = [\"xs_model\",\"s_model\",\"m_model\",\"inception_model\",\"mobile_model\"]\n",
        "optims = [\"sgd\",\"adam\",\"rmsprob\",\"adamax\"]\n",
        "lrs = [0.001,0.0001,0.00001,0.000001]\n",
        "opt_list = list(itertools.product(models,optims,lrs))\n",
        "\n",
        "# Number of models to train\n",
        "print(len(opt_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IQ5WYhBpVZ1",
        "colab_type": "code",
        "outputId": "0c85d0e2-f6c2-46b5-d885-4a739cb8ce6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start training iteration\n",
        "i = 0\n",
        "in_shape = (224,224,3)\n",
        "for combi in opt_list[:]:\n",
        "  print('Optim: '+combi[1]+', LR: '+str(combi[2]))\n",
        "  print(i)\n",
        "  model, test_it, scaler = run_test_harness(\"iterated nn\",\"missing\",combi[1],combi[0],data_type='viirs_night',lr=combi[2],batch_size=16,epochs=1000,target_size=in_shape[0], pred='nuts_diff')\n",
        "  i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtvC5NFwU0T3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of ee_import.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}