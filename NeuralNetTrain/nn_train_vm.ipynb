{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcCwLDmWkrHi"
   },
   "source": [
    "# Script To Train CNN on Google VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is mounted\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "CwZd9v4Ukxt6",
    "outputId": "7c24ccac-9e9f-4079-e548-629068f33932"
   },
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5bvu7lRgpgS"
   },
   "source": [
    "## Fit Linear Model for Benchmarking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "b3xDa2R4gohV",
    "outputId": "beb4dbb2-c711-4b4d-fd46-2cc3ff069bf6"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/data/training/'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eeb291abe7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/data/training/'"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join, getsize\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define paths to image data\n",
    "train_path = '/home/jupyter/data/training/'\n",
    "val_path = '/home/jupyter/data/validation/'\n",
    "test_path = '/home/jupyter/data/test/'\n",
    "\n",
    "# Get files\n",
    "train_files = [f for f in listdir(train_path) if isfile(join(train_path, f))]\n",
    "val_files = [f for f in listdir(val_path) if isfile(join(val_path, f))]\n",
    "test_files = [f for f in listdir(test_path) if isfile(join(test_path, f))]\n",
    "\n",
    "# Load GDP data\n",
    "y_dat = pd.read_csv(\"/home/jupyter/data/enhanced_gdp_data.csv\")\n",
    "#test_f = val_files + test_files\n",
    "test_f = test_files\n",
    "\n",
    "# Empty lists for different values\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train_diff = []\n",
    "y_test_diff = []\n",
    "y_train_abs = []\n",
    "y_test_abs = []\n",
    "\n",
    "# Get necessary info from filenames and append to list\n",
    "for f in train_files:\n",
    "  region = f.rsplit('_')[0]\n",
    "  year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
    "  mean = float(f.rsplit('_')[1])\n",
    "  X_train.append(mean)\n",
    "  y_train_diff.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_diff'].values[0])\n",
    "  y_train_abs.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_value'].values[0])\n",
    "\n",
    "# Get necessary info from filenames and append to list\n",
    "for f in test_f:\n",
    "  region = f.rsplit('_')[0]\n",
    "  year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
    "  mean = float(f.rsplit('_')[1])\n",
    "  X_test.append(mean)\n",
    "  y_test_diff.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_diff'].values[0])\n",
    "  y_test_abs.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),'nuts_value'].values[0])\n",
    "\n",
    "# Convert to numpy array\n",
    "X_train = np.array([X_train])\n",
    "X_test = np.array([X_test])\n",
    "y_train_abs = np.array([y_train_abs])\n",
    "y_test_abs = np.array([y_test_abs])\n",
    "y_train_diff = np.array([y_train_diff])\n",
    "y_test_diff = np.array([y_test_diff])\n",
    "\n",
    "# Check\n",
    "print('Train X: ',str(X_train[0][:3]))\n",
    "print('Test X: ',str(X_test[0][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rdCvDA_-5MBH",
    "outputId": "68446514-9ecf-4416-ee49-006412b6d137"
   },
   "outputs": [],
   "source": [
    "# Swap axes for corrent formatting\n",
    "X_train = np.swapaxes(X_train,0,-1)\n",
    "X_test = np.swapaxes(X_test,0,-1)\n",
    "y_train_abs = np.swapaxes(y_train_abs,0,-1)\n",
    "y_test_abs = np.swapaxes(y_test_abs,0,-1)\n",
    "y_train_diff = np.swapaxes(y_train_diff,0,-1)\n",
    "y_test_diff = np.swapaxes(y_test_diff,0,-1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "oGPIPG1n2dC-",
    "outputId": "342f8464-f8a9-4a1c-ee24-68c5ec517139"
   },
   "outputs": [],
   "source": [
    "# Perfom linear regression for relative GDPs\n",
    "reg = LinearRegression().fit(X_train, y_train_diff)\n",
    "r_square = reg.score(X_train, y_train_diff)\n",
    "print('R Square Difference: ',str(r_square))\n",
    "\n",
    "# Get predictions\n",
    "pred = reg.predict(X_test)\n",
    "\n",
    "# Get prediction accuracy\n",
    "mse = np.mean((y_test_diff-pred)**2)\n",
    "mae = np.mean(np.abs(y_test_diff-pred))\n",
    "print('MSE Difference: ',str(mse))\n",
    "print('MAE Difference: ',str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "pK7KIqWap5lg",
    "outputId": "2b51d69f-eadd-4fed-db58-a35c7a1bac11"
   },
   "outputs": [],
   "source": [
    "# Perfom linear regression for absolute GDPs\n",
    "reg = LinearRegression().fit(X_train, y_train_abs)\n",
    "r_square = reg.score(X_train, y_train_abs)\n",
    "print('R Square Absolute: ',str(r_square))\n",
    "\n",
    "# Get predictions\n",
    "pred = reg.predict(X_test)\n",
    "\n",
    "# Get prediction accuracy\n",
    "mse = np.mean((y_test_abs-pred)**2)\n",
    "mae = np.mean(np.abs(y_test_abs-pred))\n",
    "print('MSE Absolute: ',str(mse))\n",
    "print('MAE Absolute: ',str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c50RrTy6IHnt"
   },
   "source": [
    "## Sample CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Y3Xdhdryg5R",
    "outputId": "0c08bc4b-d3c4-4de3-c13d-f5f3fdb670fd"
   },
   "outputs": [],
   "source": [
    "# Load libraries to train CNN models with Tensorflow\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GaussianDropout\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.optimizers import SGD, Adam, Nadam, Adamax, Adadelta, RMSprop\n",
    "from keras.applications import VGG19, ResNet50, ResNet50V2, InceptionResNetV2, MobileNetV2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from os import listdir\n",
    "import itertools\n",
    "from os.path import isfile, join, getsize\n",
    "#Image.MAX_IMAGE_PIXELS = 213437725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ig2E8Zgp4RnD"
   },
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "in_shape = (224,224,3)\n",
    "out_shape = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN Models used for the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "tI39p13ywdCj",
    "outputId": "b8f50daa-af90-4bd4-fd3b-cffbfa8b7208"
   },
   "outputs": [],
   "source": [
    "# Very shallow CNN\n",
    "def xs_model(in_shape = (224,224,3), out_shape=1):\n",
    "    xs_model = Sequential()\n",
    "\n",
    "    xs_model.add(Conv2D(32, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
    "    xs_model.add(Flatten())\n",
    "    #xs_model.add(Dense(128, activation='relu'))\n",
    "    xs_model.add(Dense(units=out_shape,activation='linear'))\n",
    "    \n",
    "    return xs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "UAOgPKmfZUfN",
    "outputId": "681bfd55-534f-4f72-cc5e-6d419f0fa5d8"
   },
   "outputs": [],
   "source": [
    "# Very shallow CNN\n",
    "def s_model(in_shape = (224,224,3),out_shape=1):\n",
    "    s_model = Sequential()\n",
    "\n",
    "    s_model.add(Conv2D(32, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
    "    s_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    s_model.add(BatchNormalization())\n",
    "    s_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "    s_model.add(Flatten())\n",
    "    s_model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
    "    s_model.add(Dropout(0.5))\n",
    "    s_model.add(Dense(units=out_shape,activation='linear'))\n",
    "    \n",
    "    return s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9L86HWD1sWhR"
   },
   "outputs": [],
   "source": [
    "# Med Shallow Model\n",
    "def m_model(in_shape=(224,224,3),out_shape=1):\n",
    "    m_model = Sequential()\n",
    "\n",
    "    m_model.add(Conv2D(16, 3, padding='same', input_shape=in_shape, activation='relu'))\n",
    "    m_model.add(Conv2D(16, 3, padding='same', activation='relu'))\n",
    "    m_model.add(BatchNormalization())\n",
    "    m_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "    m_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    m_model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    m_model.add(BatchNormalization())\n",
    "    m_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "    m_model.add(Flatten())\n",
    "    m_model.add(Dense(128, activation='relu', activity_regularizer=l1(0.001)))\n",
    "    m_model.add(Dropout(0.5))\n",
    "\n",
    "    m_model.add(Dense(256, activation='relu',activity_regularizer=l1(0.001)))\n",
    "    m_model.add(Dropout(0.5))\n",
    "    m_model.add(Dense(units=out_shape,activation='linear'))\n",
    "    \n",
    "    return m_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JNEUN3yv3BDF",
    "outputId": "030df3bd-310e-4026-ae1e-c09c81a13a8a"
   },
   "outputs": [],
   "source": [
    "# Transfer Learning Mobile Model\n",
    "def mobile_model(in_shape=(224,224,3),out_shape=1):\n",
    "    base_model = MobileNetV2(input_shape=in_shape,include_top=False)\n",
    "\n",
    "    x=base_model.output\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(32,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "    x=Dropout(0.2)(x) #Dropout\n",
    "    x=Dense(32,activation='relu')(x) #dense layer 3\n",
    "    x=Dropout(0.2)(x) #Dropout\n",
    "    preds=Dense(units=out_shape, activation = 'linear')(x)\n",
    "    mobile_model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "    for layer in mobile_model.layers[:-26]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return mobile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "kpKmZY683B5U",
    "outputId": "23d7155a-5550-47ce-b952-1f8d3d52683d"
   },
   "outputs": [],
   "source": [
    "# Transfer Learning Inception Model\n",
    "def inception_model(in_shape=(224,224,3),out_shape=1):\n",
    "    base_model = InceptionResNetV2(input_shape=in_shape,include_top=False)\n",
    "\n",
    "    x=base_model.output\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(32,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "    x=Dropout(0.2)(x) #Dropout\n",
    "    x=Dense(32,activation='relu')(x) #dense layer 3\n",
    "    x=Dropout(0.2)(x) #Dropout\n",
    "    preds=Dense(units=out_shape, activation = 'linear')(x)\n",
    "    inception_model=Model(inputs=base_model.input,outputs=preds)\n",
    "    \n",
    "    return inception_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQnkjk-Jje67"
   },
   "source": [
    "# Training Pipeline for CNN Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6S35UIOJN9E"
   },
   "outputs": [],
   "source": [
    "# Plot histogram of image size in different sets\n",
    "train_path = \"/home/jupyter/data/training/\"\n",
    "val_path = \"/home/jupyter/data/validation/\"\n",
    "test_path = \"/home/jupyter/data/test/\"\n",
    "\n",
    "train_files = [getsize(join(train_path, f)) for f in listdir(train_path) if isfile(join(train_path, f))]\n",
    "val_files = [getsize(join(val_path, f)) for f in listdir(val_path) if isfile(join(val_path, f))]\n",
    "test_files = [getsize(join(test_path, f)) for f in listdir(test_path) if isfile(join(test_path, f))]\n",
    "\n",
    "pyplot.hist(train_files,bins=1000)\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.hist(val_files,bins=1000)\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.hist(test_files,bins=1000)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline of functions to automate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JzEKWvkUjTD"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame to load the data\n",
    "def data_load(data_type, pred = 'nuts_value'):\n",
    "  \n",
    "  # Define path to image files\n",
    "  train_path = \"/home/jupyter/data/training/\"\n",
    "  val_path = \"/home/jupyter/data/validation/\"\n",
    "  test_path = \"/home/jupyter/data/test/\"\n",
    "\n",
    "  # Get files\n",
    "  train_files = [f for f in listdir(train_path) if isfile(join(train_path, f)) and getsize(join(train_path, f)) > 100]\n",
    "  val_files = [f for f in listdir(val_path) if isfile(join(val_path, f)) and getsize(join(val_path, f)) > 100]\n",
    "  test_files = [f for f in listdir(test_path) if isfile(join(test_path, f)) and getsize(join(test_path, f)) > 100]\n",
    "\n",
    "  # NUT USED - for time series experiments\n",
    "  if pred == 'diff':\n",
    "    y_dat = pd.read_csv(\"/home/jupyter/data/gdp_time_series.csv\")\n",
    "  else:\n",
    "    y_dat = pd.read_csv(\"/home/jupyter/enhanced_gdp_data.csv\")\n",
    "\n",
    "  train_y = []\n",
    "  val_y = []\n",
    "  test_y = []\n",
    "\n",
    "  # Get the paths and GDP values for training data\n",
    "  for f in train_files:\n",
    "    if pred == 'diff':\n",
    "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
    "      region = f.rsplit('_')[0]\n",
    "      train_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
    "    else:\n",
    "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
    "      region = f.rsplit('_')[0]\n",
    "      train_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
    "\n",
    "  # Get the paths and GDP values for validation data\n",
    "  for f in val_files:\n",
    "    if pred == 'diff':\n",
    "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
    "      region = f.rsplit('_')[0]\n",
    "      val_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
    "    else:\n",
    "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
    "      region = f.rsplit('_')[0]\n",
    "      val_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
    "\n",
    "  # Get the paths and GDP values for test data\n",
    "  for f in test_files:\n",
    "    if pred == 'diff':\n",
    "      year = f.rsplit('_')[1].rsplit('.')[0]\n",
    "      region = f.rsplit('_')[0]\n",
    "      test_y.append(y_dat.loc[(y_dat['nuts2']==region),str('diff_'+year)].values[0])\n",
    "    else:\n",
    "      year = int(f.rsplit('_')[2].rsplit('.')[0])\n",
    "      region = f.rsplit('_')[0]\n",
    "      test_y.append(y_dat.loc[(y_dat['nuts2']==region)&(y_dat['year']==year),pred].values[0])\n",
    "\n",
    "  # Scale the output GDP data\n",
    "  train_y = np.array(train_y)\n",
    "  train_y = train_y.reshape(len(train_y),1)\n",
    "  val_y = np.array(val_y)\n",
    "  val_y = val_y.reshape(len(val_y),1)\n",
    "  test_y = np.array(test_y)\n",
    "  test_y = test_y.reshape(len(test_y),1)\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(train_y)\n",
    "  train_y = scaler.transform(train_y)\n",
    "  val_y = scaler.transform(val_y)\n",
    "  test_y = scaler.transform(test_y)\n",
    "  \n",
    "  # Put info into DF so that Keras can read in the data automatically\n",
    "  train_df = pd.DataFrame({'y':train_y[:,0].tolist(),'file':train_files})\n",
    "  val_df = pd.DataFrame({'y':val_y[:,0].tolist(),'file':val_files})\n",
    "  test_df = pd.DataFrame({'y':test_y[:,0].tolist(),'file':test_files})\n",
    "\n",
    "  # Return DF per split and the scaler for later\n",
    "  return train_df, val_df, test_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIsyGb6Qx2uB"
   },
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(lr,in_shape, out_shape,optim,model):\n",
    "\t\n",
    "    # Define new model\n",
    "    mod = Sequential()\n",
    "    if model == 'xs_model':\n",
    "        mod = xs_model(in_shape=in_shape)\n",
    "    elif model == 's_model':\n",
    "        mod = s_model(in_shape=in_shape)\n",
    "    elif model == 'm_model':\n",
    "        mod = m_model(in_shape=in_shape)\n",
    "    elif model == 'inception_model':\n",
    "        mod = inception_model(in_shape=in_shape)\n",
    "    else:\n",
    "        mod = mobile_model(in_shape=in_shape)\n",
    "        \n",
    "    \n",
    "    # compile model with different optimising functions and learning rates\n",
    "    if optim == 'adam':\n",
    "        opt = Adam(lr=lr)\n",
    "    elif optim == 'sgd':\n",
    "        opt = SGD(lr=lr)\n",
    "    elif optim == 'rmsprob':\n",
    "        opt = RMSprop(lr=lr, rho=0.9)\n",
    "    else:\n",
    "        opt = Adamax(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    mod.compile(optimizer=opt, loss='mean_squared_error', metrics=['mse','mae'])\n",
    " \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kg3xSQBLyujR"
   },
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history,no_layers,in_shape,batch_size,epochs,mse,mae,transfer,lr,optim,comment,data_type,pred):\n",
    "\t# Save the Diagnostics and Training plot to drive if mounted, otherwise comment out\n",
    "\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Model MSE')\n",
    "\tpyplot.plot(history.history['mean_squared_error'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_mean_squared_error'], color='orange', label='val')\n",
    "\tpyplot.xlabel(\"\")\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Model MAE')\n",
    "\tpyplot.plot(history.history['mean_absolute_error'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_mean_absolute_error'], color='orange', label='val')\n",
    "\t# save plot to file\n",
    "\tday = date.today()\n",
    "\tfilename = str(day)+\"_\"+str(in_shape[0])+\"_\"+str(in_shape[2])+\"_\"+str(epochs)+\"_\"+str(batch_size)\n",
    "\tpyplot.savefig('/gdrive/My Drive/ThesisData/cnn_results/'+filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "  \n",
    "\t# write diagnostics to results file\n",
    "\tmyrow = ['\\n'+str(day),str(no_layers),str(batch_size),str(epochs),str(in_shape[0]),str(in_shape[2]),\n",
    "\t         str(history.history['mse'][-1]),str(history.history['mae'][-1]),\n",
    "\t\t\t\t\t     str(history.history['val_mse'][-1]),str(history.history['val_mae'][-1]),\n",
    "\t\t\t\t\t\t\t   str(mse),str(mae),str(transfer),str(lr),str(optim),str(comment),str(data_type),str(pred)]\n",
    "\t\n",
    "\tmyrow = ','.join(myrow)\n",
    "\tfilepath = '/home/jupyter/cnn_results_diff.csv'\n",
    "\twith open(filepath,'a') as fd:\n",
    "\t\tfd.write(myrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_B8Hp4kcz3yK"
   },
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness(comment,transfer,optim,model,data_type,lr=0.0001,batch_size = 10,epochs = 50, target_size = 128, pred = 'nuts_value'):\n",
    "  \n",
    "  # load dataset\n",
    "  train_df, val_df, test_df, scaler = data_load(data_type=data_type, pred = pred)\n",
    "  \n",
    "  # create data generator\n",
    "  train_datagen = ImageDataGenerator()#rescale=(1.0/255))\n",
    "  val_datagen = ImageDataGenerator()#rescale=(1.0/255))\n",
    "  test_datagen = ImageDataGenerator()#rescale=(1.0/255))\n",
    "\n",
    "  # Define early stopping\n",
    "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=30, restore_best_weights=True)\n",
    "\n",
    "  # File Directories\n",
    "  train_path = \"/home/jupyter/data/training/\"\n",
    "  val_path = \"/home/jupyter/data/validation/\"\n",
    "  test_path = \"/home/jupyter/data/test/\"\n",
    "\n",
    "  # Infer color mode and input shape - only night was used\n",
    "  if 'day' in data_type or 'night' in data_type or data_type == 'subsample':\n",
    "    color_mode = 'rgb'\n",
    "    in_shape = (target_size,target_size,3)\n",
    "  else:\n",
    "    color_mode = 'rgba'\n",
    "    in_shape = (target_size,target_size,4)\n",
    "\n",
    "  print(in_shape)\n",
    "\n",
    "  # prepare iterators and give them the prepared DF with image paths and GDP values\n",
    "  train_it = train_datagen.flow_from_dataframe(train_df,\n",
    "                                               directory=train_path,\n",
    "                                               x_col = 'file',\n",
    "                                               y_col = 'y',\n",
    "                                               target_size = (target_size,target_size),\n",
    "                                               batch_size = batch_size,\n",
    "                                               class_mode = 'raw',\n",
    "                                               color_mode = color_mode)\n",
    "  \n",
    "  val_it = val_datagen.flow_from_dataframe(val_df, \n",
    "                                           directory=val_path,\n",
    "                                           x_col = 'file',\n",
    "                                           y_col = 'y',\n",
    "                                           target_size = (target_size,target_size),\n",
    "                                           batch_size = batch_size,\n",
    "                                           class_mode = 'raw',\n",
    "                                           color_mode = color_mode)\n",
    "  \n",
    "  test_it = test_datagen.flow_from_dataframe(test_df, \n",
    "                                             directory=test_path,\n",
    "                                             x_col = 'file',\n",
    "                                             y_col = 'y',\n",
    "                                             target_size = (target_size,target_size),\n",
    "                                             batch_size = batch_size,\n",
    "                                             class_mode = 'raw',\n",
    "                                             color_mode = color_mode)\n",
    "  \n",
    "  # define model\n",
    "  transfer = model\n",
    "  print('Model: '+model)\n",
    "  model = define_model(lr=lr,in_shape=in_shape,out_shape=1,optim=optim,model=model)\n",
    "  \n",
    "  # Get number of layers\n",
    "  no_layers = len(model.layers)\n",
    "  \n",
    "  # fit model\n",
    "  history = model.fit_generator(train_it, \n",
    "                                steps_per_epoch=len(train_it),\n",
    "                                validation_data=val_it, \n",
    "                                validation_steps=len(val_it),\n",
    "                                callbacks = [es], \n",
    "                                epochs=epochs, \n",
    "                                verbose=2)\n",
    "  \n",
    "  # evaluate model on unscaled GDP values\n",
    "  test_preds = model.predict_generator(test_it, steps=len(test_it), verbose=0)\n",
    "  # Inverse transformation to get unscaled predictions\n",
    "  unscaled_preds = scaler.inverse_transform(test_preds)\n",
    "  \n",
    "  # Get true GDP values\n",
    "  true_vals = test_it.labels\n",
    "  # Inverse transformation for true values\n",
    "  true_vals = scaler.inverse_transform(true_vals)\n",
    "  \n",
    "  # Compute unscaled MSE and MAE\n",
    "  mse = np.mean((true_vals-unscaled_preds)**2)\n",
    "  mae = np.mean(abs(true_vals-unscaled_preds))\n",
    "  print('> mse=%.3f, mae=%.3f' % (mse, mae))\n",
    "  \n",
    "  # Save training plot and diagnostics\n",
    "  summarize_diagnostics(history=history,\n",
    "                        no_layers=no_layers,\n",
    "                        in_shape = in_shape,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=len(history.history['loss']),\n",
    "                        mse=mse,mae=mae,\n",
    "                        transfer=transfer,lr=lr,\n",
    "                        optim=optim,\n",
    "                        comment=comment,\n",
    "                        data_type=data_type,\n",
    "                        pred = pred)\n",
    "  \n",
    "  return model, test_it, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RT5HZlld6eLT",
    "outputId": "ddda626c-1345-4862-be53-3444abf0ed17"
   },
   "outputs": [],
   "source": [
    "# Prepare iteration with all models, optimiser, and learning rates\n",
    "models = [\"xs_model\",\"s_model\",\"m_model\",\"inception_model\",\"mobile_model\"]\n",
    "optims = [\"sgd\",\"adam\",\"rmsprob\",\"adamax\"]\n",
    "lrs = [0.001,0.0001,0.00001,0.000001]\n",
    "opt_list = list(itertools.product(models,optims,lrs))\n",
    "\n",
    "# Number of models to train\n",
    "print(len(opt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_IQ5WYhBpVZ1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0c85d0e2-f6c2-46b5-d885-4a739cb8ce6a"
   },
   "outputs": [],
   "source": [
    "# Start training iteration\n",
    "i = 0\n",
    "in_shape = (224,224,3)\n",
    "for combi in opt_list[:]:\n",
    "  print('Optim: '+combi[1]+', LR: '+str(combi[2]))\n",
    "  print(i)\n",
    "  model, test_it, scaler = run_test_harness(\"iterated nn\",\"missing\",combi[1],combi[0],data_type='viirs_night',lr=combi[2],batch_size=16,epochs=1000,target_size=in_shape[0], pred='nuts_diff')\n",
    "  i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Prediction Results for best CNNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Prediction Analysis - Relative\n",
    "i = 0\n",
    "test_preds = []\n",
    "test_files = []\n",
    "test_true = []\n",
    "\n",
    "# Repeat 30 times\n",
    "while i < 30:\n",
    "\n",
    "    print('Iteration: '+str(i))\n",
    "    \n",
    "    # Train best CNN model for absolute GDP\n",
    "    model, test_it, scaler = run_test_harness(\"iterated nn\",\"missing\",'adam','mobile_model',data_type='viirs_night',lr=0.0001,batch_size=8,epochs=1000,target_size=in_shape[0], pred='nuts_diff')\n",
    "    \n",
    "    # Rescale results\n",
    "    temp_preds = model.predict_generator(test_it)\n",
    "    temp_preds = scaler.inverse_transform(temp_preds)\n",
    "    temp_files = test_it.filenames\n",
    "    temp_true = test_it.labels\n",
    "    temp_true = scaler.inverse_transform(temp_true)\n",
    "    \n",
    "    # Add results to lists\n",
    "    test_preds.extend(list(temp_preds[:,0]))\n",
    "    test_files.extend(list(temp_files))\n",
    "    test_true.extend(list(temp_true))\n",
    "    i = i+1\n",
    "    \n",
    "    # Transform all into a DF\n",
    "    res_dict = {'test_preds':test_preds, 'test_files':test_files, 'test_true_vals':test_true}\n",
    "    result_dat = pd.DataFrame(res_dict)\n",
    "\n",
    "    # Get Country\n",
    "    result_dat['country'] = result_dat['test_files'].str[:2]\n",
    "\n",
    "    # Compute Error\n",
    "    result_dat['se'] = (result_dat['test_true_vals']-result_dat['test_preds'])**2\n",
    "    result_dat['abs_error'] = abs(result_dat['test_true_vals']-result_dat['test_preds'])\n",
    "    \n",
    "    # Country level statistics\n",
    "    country_dat = result_dat.groupby(['country']).agg({'se':'mean','abs_error':'mean','test_files':'size'}).reset_index()\n",
    "    \n",
    "    # Save\n",
    "    result_dat.to_csv(\"result_preds_relative.csv\", index=False)\n",
    "    country_dat.to_csv(\"result_country_relative.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Prediction Analysis - Relative\n",
    "i = 0\n",
    "test_preds = []\n",
    "test_files = []\n",
    "test_true = []\n",
    "\n",
    "# Repeat 30 times\n",
    "while i < 30:\n",
    "    print('Iteration: '+str(i))\n",
    "    \n",
    "    # Train best model for relative GDP\n",
    "    model, test_it, scaler = run_test_harness(\"detailed analysis\",\"missing\",\"rmsprob\",\"inception_model\",data_type='viirs_night',lr=0.00001,batch_size=8,epochs=1000,target_size=in_shape[0], pred='nuts_value')\n",
    "    \n",
    "    # Rescale results\n",
    "    temp_preds = model.predict_generator(test_it)\n",
    "    temp_preds = scaler.inverse_transform(temp_preds)\n",
    "    temp_files = test_it.filenames\n",
    "    temp_true = test_it.labels\n",
    "    temp_true = scaler.inverse_transform(temp_true)\n",
    "    \n",
    "    # Save to lists\n",
    "    test_preds.extend(list(temp_preds[:,0]))\n",
    "    test_files.extend(list(temp_files))\n",
    "    test_true.extend(list(temp_true))\n",
    "    i = i+1\n",
    "    \n",
    "    # Transform all into a DF\n",
    "    res_dict = {'test_preds':test_preds, 'test_files':test_files, 'test_true_vals':test_true}\n",
    "    result_dat = pd.DataFrame(res_dict)\n",
    "\n",
    "    # Get Country\n",
    "    result_dat['country'] = result_dat['test_files'].str[:2]\n",
    "\n",
    "    # Compute Error\n",
    "    result_dat['se'] = (result_dat['test_true_vals']-result_dat['test_preds'])**2\n",
    "    result_dat['abs_error'] = abs(result_dat['test_true_vals']-result_dat['test_preds'])\n",
    "    \n",
    "    # Compute country stats\n",
    "    country_dat = result_dat.groupby(['country']).agg({'se':'mean','abs_error':'mean','test_files':'size'}).reset_index()\n",
    "    \n",
    "    # Save\n",
    "    result_dat.to_csv(\"result_preds_abs.csv\", index=False)\n",
    "    country_dat.to_csv(\"result_country_abs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of ee_import.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}